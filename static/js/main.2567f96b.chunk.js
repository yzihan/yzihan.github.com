(this["webpackJsonpzihan-site"]=this["webpackJsonpzihan-site"]||[]).push([[0],{283:function(e,t,n){"use strict";n.r(t);var i,a=n(0),c=n(1),s=n.n(c),r=n(38),o=(n(57),n(24)),l=n(10),d=n(22),h=n(14),j=n(29),b=n(4),g=n(7),u=n.p+"static/media/placeholder.19e90386.gif",p=n.p+"static/media/loading.ccdf9915.gif",m=n(288),O=n(286),x=n(12),f=n.p+"static/media/logo.103b5fa1.svg",y=n(42),v=n(287),w=[600,800,1e3,1200,1/0],k=function(e){return w[e]},S=function(e){return e&&w[e-1]||300},C=s.a.createContext(0);function A(e){var t=e.value,n=e.args,i=e.varName,c=Object(d.a)(e,["value","args","varName"]);return Object(a.jsx)(v.a,Object(l.a)(Object(l.a)({language:"javascript",showLineNumbers:!0},c),{},{children:(i?i+" = ":"")+JSON.stringify.apply(JSON,[t].concat(Object(y.a)(n||[null,2])))}))}var E={"":function(){return Object(a.jsxs)(ne,{title:"Debug Page",children:[Object(a.jsx)(D,{}),Object(a.jsxs)("p",{children:["Width Level: ",Object(a.jsx)(C.Consumer,{children:function(e){return e.toString()}})]}),Object(a.jsx)(A,{value:Object({NODE_ENV:"production",PUBLIC_URL:"",WDS_SOCKET_HOST:void 0,WDS_SOCKET_PATH:void 0,WDS_SOCKET_PORT:void 0,FAST_REFRESH:!0}),varName:"process.env"})]})},pdf:function(){L(),Object(c.useRef)(null);var e=Object(c.useState)("/data/cv.pdf"),t=Object(o.a)(e,2),n=t[0];t[1];return Object(a.jsxs)(ne,{title:"PDF Debug",children:[Object(a.jsx)(D,{}),Object(a.jsxs)("fieldset",{children:[Object(a.jsx)("legend",{children:"PDF Data"}),void 0,Object(a.jsx)("a",{href:n,target:"_blank",rel:"noreferrer",children:"Open in new window"})]}),"Rendered:"]})},default:function(){var e=W();return Object(a.jsx)(ne,{title:"React App",className:e.outerContainer,children:Object(a.jsxs)("header",{className:e.app,children:[Object(a.jsx)("img",{src:f,className:[e.appLogo,e.appLogoSpining].join(" "),alt:"logo"}),Object(a.jsxs)("p",{children:["Edit ",Object(a.jsx)("code",{children:"src/App.tsx"})," and save to reload."]}),Object(a.jsx)(b.b,{to:"/debug/test",className:e.appLink,children:"View Test"}),Object(a.jsxs)(b.b,{to:"/debug/",className:e.appLink,children:["Back to ",Object(a.jsx)("code",{children:"/debug/"})]})]})})},test:function(){var e=W();return Object(a.jsx)(ne,{title:"Hello, React App!",className:e.outerContainer,children:Object(a.jsxs)("header",{className:e.app,children:[Object(a.jsx)("img",{src:p,className:e.appLogo,alt:"logo"}),Object(a.jsx)("p",{children:"This is test page!"}),Object(a.jsx)(b.b,{to:"/debug/default",className:e.appLink,children:"View Debug Page"}),Object(a.jsxs)(b.b,{to:"/debug/",className:e.appLink,children:["Back to ",Object(a.jsx)("code",{children:"/debug/"})]})]})})}};function T(){var e=[];for(var t in E){var n="/debug/".concat(t);e.push(Object(a.jsx)(g.b,{exact:!0,path:n,component:E[t]},n))}return e.push(Object(a.jsx)(g.b,{path:"/debug/",render:function(){return Object(a.jsx)(g.a,{to:"/debug/"})}},"wildcard")),e}function D(e){var t=[];for(var n in E){var i="/debug/".concat(n);t.push(Object(a.jsx)("li",{children:Object(a.jsx)(b.b,Object(l.a)(Object(l.a)({},e),{},{to:i,children:i}))},i))}return Object(a.jsxs)(s.a.Fragment,{children:[Object(a.jsx)("h2",{style:{marginTop:0},children:"Debug Pages"}),Object(a.jsx)(b.b,Object(l.a)(Object(l.a)({},e),{},{to:"/",children:"Back to app index"})),Object(a.jsx)("ul",{children:t})]})}var L=Object(x.a)({viewer:{maxWidth:"600px",margin:[[0,"auto"]]}});var W=Object(x.a)({"@keyframes App-logo-spin":{from:{transform:"rotate(0deg)"},to:{transform:"rotate(360deg)"}},appLogo:Object(h.a)({height:"40vmin",pointerEvents:"none"},"@media (max-width: ".concat(S(0),"px)"),{height:.4*S(0)}),appLogoSpining:{"@media (prefers-reduced-motion: no-preference)":{animation:"$App-logo-spin infinite 20s linear"}},outerContainer:{backgroundColor:"#282c34"},app:(i={minHeight:"100vh",display:"flex",flexDirection:"column",alignItems:"center",justifyContent:"center",fontSize:"calc(10px + 2vmin)"},Object(h.a)(i,"@media (max-width: ".concat(S(0),"px)"),{fontSize:10+.02*S(0)}),Object(h.a)(i,"color","white"),Object(h.a)(i,"textAlign","center"),i),appLink:{color:"#61dafb"}});Object(x.a)({outerContainer:{},page:{background:"white",color:"black",textAlign:"center",display:"flex",flexDirection:"column",justifyContent:"center",padding:[[0,30]]},infoBlock:{textAlign:"left"}});var P=n.p+"static/media/bio.61dbeff3.png",Y=n.p+"static/media/argaze.50a16d28.jpg",N=n.p+"static/media/towards.92470a50.jpg",I=n.p+"static/media/reducing.45899bf7.jpg",R=n.p+"static/media/gender.949a3786.jpg",Z=n.p+"static/media/bamboo.2bc0fdde.jpg",z=n.p+"static/media/tibetan.99e60c33.jpg",H=n.p+"static/media/headset.879e6c92.jpg",F=n.p+"static/media/argaze2.0074dab2.png",U=n.p+"static/media/towards2.6021ae32.png",B=n.p+"static/media/reducing2.eb43435f.png",G=n.p+"static/media/gender2.ef8f46c2.png",X=n.p+"static/media/bamboo2.55ae7ba6.png";var q,J={"/":{title:"Home",shortTitle:"Home",component:function(){return Object(a.jsxs)(ne,{title:"Home",children:[Object(a.jsx)("h2",{children:"Bio"}),Object(a.jsxs)(V,{src:P,alt:"bio",allowLargeImage:!0,children:[Object(a.jsx)("p",{children:"Hello, I\u2019m Zihan Yan!"}),Object(a.jsx)("p",{children:"I am an undergraduate from Zhejiang University, major in industrial design of Computer Science College and minor in the advanced education engineering class of CHU KOCHEN Honors college. My research interests lie in human-computer interaction(augmented reality, cross-object/screen interaction), ubiquitous computing (deep learning, sensing wearable devices), and cognitive psychology (cognitive load, mind wandering). Coming from multi-disciplinary backgrounds, I pursue to further facilitate human-to-human communication by combining machine intelligence and artificial intelligence."}),Object(a.jsxs)("p",{children:["Presently, I am a research intern at ",Object(a.jsx)("a",{href:"https://www.scifilab.org/",children:"SciFi Lab"})," of Cornell University advised by ",Object(a.jsx)("a",{href:"http://www.czhang.org/",children:"Prof. Cheng Zhang"}),". During the summer vacation of my sophomore year, I was previously a student intern at ",Object(a.jsx)("a",{href:"http://www.nus-hci.org/",children:"NUS-HCI Lab"})," of National University of Singapore under the guidance of ",Object(a.jsx)("a",{href:"http://www.shengdongzhao.com",children:"Prof. Shengdong Zhao"}),". In addition, I was a research assistant in CDC-Lab of Zhejiang University and completed some researches with the supervisor ",Object(a.jsx)("a",{href:"https://person.zju.edu.cn/en/andolxli",children:"Prof. Xiangdong Li"})," and co-advisor ",Object(a.jsx)("a",{href:"https://hansen.blogs.dsv.su.se/",children:"Prof. Preben Hansen"})," from Stockholm University."]})]}),Object(a.jsx)("h2",{children:"Publications"}),Object(a.jsxs)(V,{src:Y,alt:"argaze",children:[Object(a.jsx)(b.b,{to:"/publications/argaze.html",children:Object(a.jsx)("h3",{children:"ARGaze: A Dataset of Eye Gaze Images for Calibration-Free Eye Tracking with Augmented Reality Headset"})}),Object(a.jsxs)("p",{children:[Object(a.jsx)("span",{style:{fontWeight:"bold"},children:"Zihan Yan"}),", Yue Wu, Yifei Shan, Wenqian Chen, Xiangdong Li",Object(a.jsx)("br",{}),"On reviewing ",Object(a.jsx)("span",{style:{display:"inline-block",float:"right"},children:"Scientific Data - Nature"})]})]}),Object(a.jsxs)(V,{src:N,alt:"towards",children:[Object(a.jsx)(b.b,{to:"/publications/towards.html",children:Object(a.jsx)("h3",{children:"Towards moment-to-moment attention-aware interfaces by detecting subsecond-scale attention fluctuations through EEG"})}),Object(a.jsxs)("p",{children:["Shan Zhang*, ",Object(a.jsx)("span",{style:{fontWeight:"bold"},children:"Zihan Yan*"}),", Shardul Sapkota, Shengdong Zhao, Wei-Tsang Ooi, Ye Qiyuan",Object(a.jsx)("br",{}),"On reviewing",Object(a.jsx)("span",{style:{display:"inline-block",float:"right"},children:"CHI2021"})]})]}),Object(a.jsxs)(V,{src:I,alt:"reducing",children:[Object(a.jsx)(b.b,{to:"/publications/reducing.html",children:Object(a.jsx)("h3",{children:"Reducing Cognitive Loads in Parcel Scanning with Eye Tracking-based Augmented Reality Headset"})}),Object(a.jsxs)("p",{children:[Object(a.jsx)("span",{style:{fontWeight:"bold"},children:"Zihan Yan"}),", Yufei Wu, Yiyang Li, Yifei Shan, Preben Hansen, Xiangdong Li",Object(a.jsx)("br",{}),"On reviewing",Object(a.jsx)("span",{style:{display:"inline-block",float:"right"},children:"CHI2021"})]})]}),Object(a.jsxs)(V,{src:R,alt:"gender",children:[Object(a.jsx)(b.b,{to:"/publications/gender.html",children:Object(a.jsx)("h3",{children:"Gender Differences of Cognitive Loads in Augmented Reality-based Warehouse"})}),Object(a.jsxs)("p",{children:[Object(a.jsx)("span",{style:{fontWeight:"bold"},children:"Zihan Yan"}),", Yifei Shan, Kailin Yin, Yiyang Li, Xiangdong Li",Object(a.jsx)("br",{}),"On reviewing",Object(a.jsx)("span",{style:{display:"inline-block",float:"right"},children:"IEEE VR2021"})]})]}),Object(a.jsx)("h2",{children:"Design"}),Object(a.jsx)("h3",{children:"International Competition"}),Object(a.jsxs)(V,{src:Z,alt:"bamboo",children:[Object(a.jsx)(b.b,{to:"/designs/bamboo.html",children:Object(a.jsx)("h3",{children:"Bamboo Shoot (A Soil Remediation Product Used Industrial Wastes)"})}),Object(a.jsxs)("p",{children:["Qianya Lou, Jingchen An, Kaiqi Jiang, ",Object(a.jsx)("span",{style:{fontWeight:"bold"},children:"Zihan Yan"}),Object(a.jsx)("br",{}),"iF Design Talent Award 2020"]})]}),Object(a.jsx)("h3",{children:"Patents for Invention"}),Object(a.jsxs)(V,{src:z,alt:"tibetan",children:[Object(a.jsx)("h3",{children:"A Tibetan Dance Shoe with Pattern Projection Function"}),Object(a.jsxs)("p",{children:["Ning Zou, ",Object(a.jsx)("span",{style:{fontWeight:"bold"},children:"Zihan Yan"}),", Chunlei Chai",Object(a.jsx)("br",{}),"CN 110710755 A"]})]}),Object(a.jsxs)(V,{src:H,alt:"headset",children:[Object(a.jsx)("h3",{children:"An Eye-tracking based calibration-free AR headset design for picking task in warehouse"}),Object(a.jsxs)("p",{children:["Xiangdong Li, Yuting Niu, Zhongnan Huang, ",Object(a.jsx)("span",{style:{fontWeight:"bold"},children:"Zihan Yan"}),", Yue Wu, Yifei Shan",Object(a.jsx)("br",{}),"On reviewing"]})]})]})}},"/experience.html":{title:"Experience",shortTitle:"Experience",component:function(){return Object(a.jsxs)(ne,{title:"Experience",children:[Object(a.jsx)("h2",{children:"Experience"}),Object(a.jsx)("h3",{children:"Education"}),Object(a.jsxs)("ul",{children:[Object(a.jsx)("li",{children:"Undergraduate, Zhejiang University"}),Object(a.jsxs)("li",{children:["Industrial Design in Computer Science and Technology College",Object(a.jsxs)("ul",{children:[Object(a.jsx)("li",{children:"GPA: 3.97/4 (rank 1/23)"}),Object(a.jsx)("li",{children:"Minor in Advanced Honor Class of Engineering Education, Chu Kochen Honors College (40/6000)"})]})]})]}),Object(a.jsx)("h3",{children:"Research Experience"}),Object(a.jsxs)("ul",{children:[Object(a.jsx)("li",{children:Object(a.jsx)("p",{children:Object(a.jsx)("span",{children:"Research Intern, Dept, of Computing and Information Science, Cornell University, 2020.9 - 2021.4"})})}),Object(a.jsx)("li",{children:Object(a.jsx)("p",{children:Object(a.jsx)("span",{children:"Research Intern, Dept. of Computer Science, National University of Singapore, 2020.6 - 2020.9"})})}),Object(a.jsx)("li",{children:Object(a.jsx)("p",{children:Object(a.jsx)("span",{children:"Research Assistant, College of Computer Science and Technology, Zhejiang University, 2019.12 - 2020.6"})})})]}),Object(a.jsx)("h3",{children:"Selected Awards"}),Object(a.jsxs)("ul",{children:[Object(a.jsx)("li",{children:Object(a.jsx)("p",{children:Object(a.jsx)("span",{children:"iF Design Talent Award 2020, 2020.11"})})}),Object(a.jsx)("li",{children:Object(a.jsx)("p",{children:Object(a.jsx)("span",{children:"NationalScholarship (6/404), 2020.10"})})}),Object(a.jsx)("li",{children:Object(a.jsx)("p",{children:Object(a.jsx)("span",{children:"Lixin Tang Scholarship (60/All undergraduate, master's and doctoral students of Zhejiang University), 2019.11"})})}),Object(a.jsx)("li",{children:Object(a.jsx)("p",{children:Object(a.jsx)("span",{children:"National Scholarship (1/73), 2019.10"})})}),Object(a.jsx)("li",{children:Object(a.jsx)("p",{children:Object(a.jsx)("span",{children:"Top 10 Students of Yunfeng Academy in Zhejiang University (10/2000), 2019.5"})})})]}),Object(a.jsx)("h3",{children:"Skills"}),Object(a.jsx)("h4",{children:"Coding"}),Object(a.jsxs)("ul",{children:[Object(a.jsx)("li",{children:Object(a.jsx)("p",{children:Object(a.jsx)("span",{children:"Augment Reality Development (Unity)"})})}),Object(a.jsx)("li",{children:Object(a.jsx)("p",{children:Object(a.jsx)("span",{children:"Machine Learning (PyTorch)"})})}),Object(a.jsx)("li",{children:Object(a.jsx)("p",{children:Object(a.jsx)("span",{children:"Signal Processing (OpenCV, EEGLab)"})})}),Object(a.jsx)("li",{children:"Embedded Systems: Arduino series"})]}),Object(a.jsx)("h4",{children:"Design"}),Object(a.jsxs)("ul",{children:[Object(a.jsx)("li",{children:Object(a.jsx)("p",{children:Object(a.jsx)("span",{children:"User experience (UX) design: User-Centered Design"})})}),Object(a.jsx)("li",{children:"Service Design: Journey Map, System Map, Service Blueprint"}),Object(a.jsx)("li",{children:"Plane Design: PS, AI"}),Object(a.jsx)("li",{children:"Tangible Design: Rhino, Keyshot"})]}),Object(a.jsx)("h4",{children:"Math"}),Object(a.jsxs)("ul",{children:[Object(a.jsx)("li",{children:"Mathematical modeling"}),Object(a.jsx)("li",{children:"Probability and statistics, Discrete mathematics"})]}),Object(a.jsx)("h4",{children:"Language"}),Object(a.jsx)("ul",{children:Object(a.jsx)("li",{children:Object(a.jsx)("p",{children:Object(a.jsx)("span",{children:"English (TOEFL)"})})})})]})}},"/publications/":{title:"Publication",shortTitle:"Publication",component:function(){return Object(a.jsxs)(ne,{title:"Publications",children:[Object(a.jsxs)(V,{src:Y,alt:"argaze",children:[Object(a.jsx)(b.b,{to:"/publications/argaze.html",children:Object(a.jsx)("h3",{children:"ARGaze: A Dataset of Eye Gaze Images for Calibration-Free Eye Tracking with Augmented Reality Headset"})}),Object(a.jsxs)("p",{children:[Object(a.jsx)("span",{style:{fontWeight:"bold"},children:"Zihan Yan"}),", Yue Wu, Yifei Shan, Wenqian Chen, Xiangdong Li",Object(a.jsx)("br",{}),"On reviewing ",Object(a.jsx)("span",{style:{display:"inline-block",float:"right"},children:"Scientific Data - Nature"})]})]}),Object(a.jsxs)(V,{src:N,alt:"towards",children:[Object(a.jsx)(b.b,{to:"/publications/towards.html",children:Object(a.jsx)("h3",{children:"Towards moment-to-moment attention-aware interfaces by detecting subsecond-scale attention fluctuations through EEG"})}),Object(a.jsxs)("p",{children:["Shan Zhang*, ",Object(a.jsx)("span",{style:{fontWeight:"bold"},children:"Zihan Yan*"}),", Shardul Sapkota, Shengdong Zhao, Wei-Tsang Ooi, Ye Qiyuan",Object(a.jsx)("br",{}),"On reviewing",Object(a.jsx)("span",{style:{display:"inline-block",float:"right"},children:"CHI2021"})]})]}),Object(a.jsxs)(V,{src:I,alt:"reducing",children:[Object(a.jsx)(b.b,{to:"/publications/reducing.html",children:Object(a.jsx)("h3",{children:"Reducing Cognitive Loads in Parcel Scanning with Eye Tracking-based Augmented Reality Headset"})}),Object(a.jsxs)("p",{children:[Object(a.jsx)("span",{style:{fontWeight:"bold"},children:"Zihan Yan"}),", Yufei Wu, Yiyang Li, Yifei Shan, Preben Hansen, Xiangdong Li",Object(a.jsx)("br",{}),"On reviewing",Object(a.jsx)("span",{style:{display:"inline-block",float:"right"},children:"CHI2021"})]})]}),Object(a.jsxs)(V,{src:R,alt:"gender",children:[Object(a.jsx)(b.b,{to:"/publications/gender.html",children:Object(a.jsx)("h3",{children:"Gender Differences of Cognitive Loads in Augmented Reality-based Warehouse"})}),Object(a.jsxs)("p",{children:[Object(a.jsx)("span",{style:{fontWeight:"bold"},children:"Zihan Yan"}),", Yifei Shan, Kailin Yin, Yiyang Li, Xiangdong Li",Object(a.jsx)("br",{}),"On reviewing",Object(a.jsx)("span",{style:{display:"inline-block",float:"right"},children:"IEEE VR2021"})]})]})]})}},"/publications/argaze.html":{component:function(){return Object(a.jsxs)(ne,{title:"ARGaze: A Dataset of Eye Gaze Images for Calibration-Free Eye Tracking with Augmented Reality Headset",children:[Object(a.jsx)(b.b,{to:"/publications/",children:"Show All Publications"}),Object(a.jsx)("h3",{children:"ARGaze: A Dataset of Eye Gaze Images for Calibration-Free Eye Tracking with Augmented Reality Headset"}),Object(a.jsx)("p",{children:"Zihan Yan, Yue Wu, Yifei Shan, Wenqian Chen, Xiangdong Li"}),Object(a.jsx)("img",{style:{maxWidth:"100%"},src:F,alt:"argaze2"}),Object(a.jsx)("p",{children:"Eye-tracking is a widespread method in human-computer interaction. However, it is often criticised for the troublesome calibration with new users and scenes. Despite progress in machine learning-based eye tracking, preparing a qualified dataset remains challenging. We present ARGaze, a dataset of eye gaze images, for calibration-free eye tracking with AR headset. The dataset was derived from 25 participants who conducted eye gaze tasks in augmented reality and real-world scenes for approximately 30min. It comprises 1,321,968 pairs of eye images and corresponding world view in 50 videos. To validate the dataset, we implemented the SIFTNet- and ALSTM-FCN-hybrid model and compared the results with that of the state-of-the-art research. The results show that the dataset is of high compatibility with different machine learning models and it contains sufficient eye gaze-related features that enable the record low eye gaze estimation error by 3.70degree on average and 1.56degree on specific participant, without involving any pre-study calibrations across the participants. Guidance for dataset reuse and related implications for eye tracking design and evaluation are described."}),Object(a.jsx)(b.b,{to:"/publications/",children:"Show All Publications"})]})}},"/publications/towards.html":{component:function(){return Object(a.jsxs)(ne,{title:"Towards moment-to-moment attention-aware interfaces by detecting subsecond-scale attention fluctuations through EEG",children:[Object(a.jsx)(b.b,{to:"/publications/",children:"Show All Publications"}),Object(a.jsx)("h3",{children:"Towards moment-to-moment attention-aware interfaces by detecting subsecond-scale attention fluctuations through EEG"}),Object(a.jsx)("p",{children:"Shan Zhang*, Zihan Yan*, Shardul Sapkota, Shengdong Zhao, Wei-Tsang Ooi, Ye Qiyuan"}),Object(a.jsx)("img",{style:{maxWidth:"100%"},src:U,alt:"towards2"}),Object(a.jsx)("p",{children:"Moment-to-moment attention fluctuation can bring unwanted and even life-threatening outcomes. While there are previous studies pertaining to attention detection, none have studied sub-second-scale moment-to-moment attention fluctuations. To enable automatic detection of this occurrence, we utilized a novel paradigm in psychology\u2014 the gradCPT, which measures attention through response time variability measurements and classifies sub-second-scale moment-to-moment attention fluctuations as \u201cin-the-zone\u201d and \u201cout-of-the-zone\u201d. We developed an EEG-based classifier to classify both states with an accuracy of 73.4%. We then evaluated our classifier through a thought probe study in the video learning scenario. The probe study showed that our EEG-based classifier can classify on-task and mind-wandering during video learning and improve the state-of-the-art by 9%. The results demonstrated the effectiveness of EEG-based detection of moment-to-moment attention fluctuation, which can be used to develop more attention aware systems in other real-world scenarios."}),Object(a.jsx)(b.b,{to:"/publications/",children:"Show All Publications"})]})}},"/publications/reducing.html":{component:function(){return Object(a.jsxs)(ne,{title:"Reducing Cognitive Loads in Parcel Scanning with Eye Tracking-based Augmented Reality Headset",children:[Object(a.jsx)(b.b,{to:"/publications/",children:"Show All Publications"}),Object(a.jsx)("h3",{children:"Reducing Cognitive Loads in Parcel Scanning with Eye Tracking-based Augmented Reality Headset"}),Object(a.jsx)("p",{children:"Zihan Yan, Yufei Wu, Yiyang Li, Yifei Shan, Preben Hansen, Xiangdong Li"}),Object(a.jsx)("img",{style:{maxWidth:"100%"},src:B,alt:"reducing2"}),Object(a.jsx)("p",{children:"Parcel scanning in warehouse is cognitively demanding as it involves processes e.g. barcode seeking, result confirming, and parcel relocating that have different cognitive requirements. It may deteriorate human workers\u2019 productivity and well-being of working. Despite many ergonomic solutions to parcel scanning, worker\u2019s cognition loads are less concerned. To reduce cognitive loads in parcel scanning, we present the eye tracking-based augmented reality headset which integrates foveated vision detection to leverage spontaneous barcode scanning and result confirmation. We evaluated the headset\u2019s usability and its influence on workers\u2019 cognition performance with two groups of participants using the headset and PDA, respectively. The result shows that the headset significantly reduced participants\u2019 overall cognitive loads in barcode seeking and result confirmation, it maintained higher scanning efficiency and lower cognitive loads than the PDA across all the tasks. The headset outperformed the PDA with respect to overall usability and ease of use."}),Object(a.jsx)(b.b,{to:"/publications/",children:"Show All Publications"})]})}},"/publications/gender.html":{component:function(){return Object(a.jsxs)(ne,{title:"Gender Differences of Cognitive Loads in Augmented Reality-based Warehouse",children:[Object(a.jsx)(b.b,{to:"/publications/",children:"Show All Publications"}),Object(a.jsx)("h3",{children:"Gender Differences of Cognitive Loads in Augmented Reality-based Warehouse"}),Object(a.jsx)("p",{children:"Zihan Yan, Yifei Shan, Kailin Yin, Yiyang Li, Xiangdong Li"}),Object(a.jsx)("img",{style:{maxWidth:"100%"},src:G,alt:"gender2"}),Object(a.jsx)("p",{children:"Augmented reality is emerging into contemporary warehouses and considerably improves human worker\u2019s cognition abilities and overall productivity. Despite proven benefits, little is known about how the male and female workers cope with cognitive loads of different warehouse tasks. Therefore, we developed the augmented reality headset to help the participants facilitate parcel sorting tasks and conducted empirical studies to investigate the gender differences of cognitive loads. The results show that the female workers had significantly lower operational efficiency, higher visual attention, and higher memory loads than the male, but they quickly gained advantages in all these aspects as fewer gender differences were identified with more difficult tasks. The female workers had higher visual attention than the male in the process of information seeking and interpretation. There were constant differences of working memory between the female and male throughout all the processes. Implications for the gender difference of cognitive loads are discussed."}),Object(a.jsx)(b.b,{to:"/publications/",children:"Show All Publications"})]})}},"/designs/":{title:"Design",shortTitle:"Design",component:function(){return Object(a.jsxs)(ne,{title:"Design",children:[Object(a.jsx)("h2",{children:"Design"}),Object(a.jsx)("h3",{children:"International Competition"}),Object(a.jsxs)(V,{src:Z,alt:"bamboo",children:[Object(a.jsx)(b.b,{to:"/designs/bamboo.html",children:Object(a.jsx)("h3",{children:"Bamboo Shoot (A Soil Remediation Product Used Industrial Wastes)"})}),Object(a.jsxs)("p",{children:["Qianya Lou, Jingchen An, Kaiqi Jiang, ",Object(a.jsx)("span",{style:{fontWeight:"bold"},children:"Zihan Yan"}),Object(a.jsx)("br",{}),"iF Design Talent Award 2020"]})]}),Object(a.jsx)("h3",{children:"Patents for Invention"}),Object(a.jsxs)(V,{src:z,alt:"tibetan",children:[Object(a.jsx)("h3",{children:"A Tibetan Dance Shoe with Pattern Projection Function"}),Object(a.jsxs)("p",{children:["Ning Zou, ",Object(a.jsx)("span",{style:{fontWeight:"bold"},children:"Zihan Yan"}),", Chunlei Chai",Object(a.jsx)("br",{}),"CN 110710755 A"]})]}),Object(a.jsxs)(V,{src:H,alt:"headset",children:[Object(a.jsx)("h3",{children:"An Eye-tracking based calibration-free AR headset design for picking task in warehouse"}),Object(a.jsxs)("p",{children:["Xiangdong Li, Yuting Niu, Zhongnan Huang, ",Object(a.jsx)("span",{style:{fontWeight:"bold"},children:"Zihan Yan"}),", Yue Wu, Yifei Shan",Object(a.jsx)("br",{}),"On reviewing"]})]})]})}},"/designs/bamboo.html":{component:function(){return Object(a.jsxs)(ne,{title:"Design",children:[Object(a.jsx)(b.b,{to:"/designs/",children:"Show All Designs"}),Object(a.jsx)("h3",{children:"Bamboo Shoot (A Soil Remediation Product Used Industrial Wastes)"}),Object(a.jsx)("p",{children:"Qianya Lou, Jingchen An, Kaiqi Jiang, Zihan Yan"}),Object(a.jsx)("img",{style:{maxWidth:"100%"},src:X,alt:"bamboo2"}),Object(a.jsx)("h4",{children:"Introduction"}),Object(a.jsx)("p",{children:"Bamboo shoots, which give people the hope of green life by the upward growth posture, is a design to deal with heavy metals and organic soil pollution. Nowadays, the standard rate of the national total soil point exceeding is 16.1%, so the soil recovery needs to be highlighted."}),Object(a.jsx)("h4",{children:"Jury Statement"}),Object(a.jsx)("p",{children:"Bamboo Shoot is an interesting proposal with great application potential. The proposal solves several problems related to soil restoration."}),Object(a.jsx)(b.b,{to:"/designs/",children:"Show All Designs"})]})}}},K=n(47),M=n(33),_=n(48);K.a.add(M.b,M.a,_.a);var $=[p,u],Q=Object(x.a)({img:(q={width:"40%",float:"left",height:"100%",marginRight:20},Object(h.a)(q,"@media (max-width: ".concat(k(0),"px)"),{display:"block",float:"none",width:"100%",height:"auto",maxWidth:1*S(0),margin:[[0,"auto"]]}),Object(h.a)(q,"&:not($largeImage)",{maxWidth:.66666*S(0)}),q),largeImage:{},container:{clear:"both","&>p":{textAlign:"justify",textAlignLast:"auto"},"&::after":{content:'""',display:"block",clear:"both"}}});function V(e){var t=Q(),n=e.className,i=e.children,c=e.alt,s=e.allowLargeImage,r=Object(d.a)(e,["className","children","alt","allowLargeImage"]);return Object(a.jsxs)("div",{className:t.container,children:[Object(a.jsx)("img",Object(l.a)({className:[n,t.img,s?t.largeImage:void 0].join(" "),alt:c},r)),i]})}var ee=Object(x.a)({"@global":{body:{minWidth:S(0)},a:{color:"inherit"}},originalName:{fontSize:"0.6em",fontWeight:"normal",display:"inline-block",margin:10,fontStyle:"normal","&:before":{content:'"("',display:"inline"},"&:after":{content:'")"',display:"inline"}},"@keyframes gradient":{from:{backgroundPosition:[["0%","50%"]]},"50%":{backgroundPosition:[["100%","50%"]]},to:{backgroundPosition:[["0%","50%"]]}},header:{background:"linear-gradient(-45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab)",color:"white",backgroundSize:[["400%","400%"]],animation:"$gradient 15s ease infinite",padding:40,"& h1, & p":{margin:0,padding:0}},stuck:{},navBar:{transition:"0.2s","& ul":{display:"flex",flexFlow:[["row","wrap"]],listStyleType:"none",padding:0,margin:0},"& li":{display:"inline-block"},"&:not($navBarFloating)":{background:"transparent",margin:[[20,0,-40,-20]],opacity:1,"$stuck&":{opacity:0}}},navBarFloating:{position:"fixed",left:0,top:0,right:0,zIndex:99999,background:"#000000",opacity:0,"$stuck&":{opacity:1}},navButton:{cursor:"pointer",transition:"0.1s",background:"transparent",display:"inline-block",padding:[[15,20]],"&:hover":{background:"rgba(204,204,204,0.5)"}},framework:{},footer:{background:"linear-gradient(to bottom right, #50a3a2 0%, #53e3a6 100%)",fallback:{background:"#50a3a2"},color:"white",position:"relative",padding:40,display:"flex",justifyContent:"space-between","&>div":{display:"block"},"& h3, & p":{marginTop:0}},footerBg:{position:"absolute",top:0,left:0,width:"100%",height:"100%",zIndex:1,pointerEvents:"none",margin:0,padding:0,overflow:"hidden","& li":{position:"absolute",listStyle:"none",display:"block",width:40,height:40,backgroundColor:"rgba(255,255,255,0.15)",bottom:-160,animation:"$square 25s infinite",transitionTimingFunction:"linear","&:nth-child(1)":{left:"10%"},"&:nth-child(2)":{left:"20%",width:80,height:80,animationDelay:"2s",animationDuration:"17s"},"&:nth-child(3)":{left:"25%",animationDelay:"4s"},"&:nth-child(4)":{left:"40%",width:60,height:60,animationDuration:"22s",backgroundColor:"rgba(255,255,255,0.25)"},"&:nth-child(5)":{left:"70%"},"&:nth-child(6)":{left:"80%",width:120,height:120,animationDelay:"3s",backgroundColor:"rgba(255,255,255,0.20)"},"&:nth-child(7)":{left:"32%",width:160,height:160,animationDelay:"7s"},"&:nth-child(8)":{left:"55%",width:20,height:20,animationDelay:"15s",animationDuration:"40s"},"&:nth-child(9)":{left:"25%",width:10,height:10,animationDelay:"2s",animationDuration:"40s",backgroundColor:"rgba(255,255,255,0.30)"},"&:nth-child(10)":{left:"90%",width:160,height:160,animationDelay:"11s"}}},"@keyframes square":{from:{transform:"translateY(0)"},to:{transform:"translateY(-700px) rotate(600deg)"}},pageContainer:{position:"relative",transition:"0.3s"},pageEnter:{position:"absolute",left:0,right:0,top:0,opacity:1},pageExit:{position:"absolute",left:0,right:0,top:0,opacity:0}}),te=Object(x.a)({outerContainer:{transition:"0.3s",minHeight:"30vh",fallback:{minHeight:"300px"},padding:20},pageContentWrapper:{maxWidth:k(2),margin:[[0,"auto"]]}});function ne(e){var t=te(),n=e.title,i=e.className,c=e.children,s=Object(d.a)(e,["title","className","children"]);return Object(a.jsx)("div",Object(l.a)(Object(l.a)({className:[t.outerContainer,i].join(" ")},s),{},{children:Object(a.jsxs)("div",{className:t.pageContentWrapper,children:[Object(a.jsx)(j.a,{title:n}),c]})}))}var ie=function(){var e=ee(),t=Object(c.useState)(document.body.clientWidth),n=Object(o.a)(t,2),i=n[0],s=n[1],r=Object(c.useRef)(null),l=Object(c.useState)(!1),d=Object(o.a)(l,2),h=d[0],u=d[1];Object(c.useEffect)((function(){var e=function(e){s(document.body.clientWidth)};return window.addEventListener("resize",e),function(){return window.removeEventListener("resize",e)}})),Object(c.useEffect)((function(){var e=function(){r.current&&u(r.current.getBoundingClientRect().top<=1)};e();var t=function(e){var t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:100,n=null;return function(){for(var i=arguments.length,a=new Array(i),c=0;c<i;c++)a[c]=arguments[c];n||(n=setTimeout((function(){e.apply(void 0,a),n=null}),t))}}(e,50);return window.addEventListener("resize",t),window.addEventListener("scroll",t),function(){window.removeEventListener("resize",t),window.removeEventListener("scroll",t)}}));var p,x=(p=i,w.findIndex((function(e){return p<=e}))),f=["Collage of Computer Science","Collage of Computer Science | Zhejiang University","Collage of Computer Science and Technology | Zhejiang University","Collage of Computer Science and Technology | Zhejiang University","Collage of Computer Science and Technology | Zhejiang University"],y=x<1?"shortTitle":"title",v=Object(a.jsx)("ul",{children:Object.keys(J).filter((function(e){return!!J[e].title})).map((function(t){return Object(a.jsx)("li",{children:Object(a.jsx)(b.b,{to:t,className:e.navButton,children:J[t][y]})},t)}))});return Object(a.jsx)(g.b,{render:function(t){var n=t.location;return Object(a.jsxs)(C.Provider,{value:x,children:[Object(a.jsxs)("div",{className:e.framework,children:[Object(a.jsx)(j.a,{children:$.map((function(e){return Object(a.jsx)("link",{rel:"prefetch",href:e},e)}))}),Object(a.jsxs)("header",{className:e.header,children:[Object(a.jsx)("h1",{children:"Zihan Yan "}),Object(a.jsx)("p",{children:f[x]}),Object(a.jsx)("nav",{ref:r,className:[e.navBar,h?e.stuck:void 0].join(" "),children:v}),Object(a.jsx)("nav",{className:[e.navBar,e.navBarFloating,h?e.stuck:void 0].join(" "),children:v})]}),Object(a.jsx)(m.a,{component:"main",className:e.pageContainer,children:Object(a.jsx)(O.a,{classNames:{enter:e.pageEnter,exit:e.pageExit},timeout:1,children:Object(a.jsxs)(g.d,{location:n,children:[T(),Object.keys(J).map((function(e){return Object(a.jsx)(g.b,{exact:!0,path:e,component:J[e].component},e)})),Object(a.jsx)(g.b,{render:function(){return Object(a.jsx)(g.a,{to:"/"})}})]},n.pathname)},n.pathname)})]}),Object(a.jsxs)("footer",{className:e.footer,children:[Object(a.jsxs)("div",{children:[Object(a.jsx)("h3",{children:"Contact Information"}),Object(a.jsx)("p",{children:Object(a.jsx)("a",{href:"mailto:zihanyan@zju.edu.cn",children:"zihanyan@zju.edu.cn"})})]}),Object(a.jsx)("div",{style:{display:void 0},children:Object(a.jsx)("p",{children:"\xa9 2020 Zihan"})}),Object(a.jsxs)("ul",{className:e.footerBg,style:{clear:"both"},children:[Object(a.jsx)("li",{}),Object(a.jsx)("li",{}),Object(a.jsx)("li",{}),Object(a.jsx)("li",{}),Object(a.jsx)("li",{}),Object(a.jsx)("li",{}),Object(a.jsx)("li",{}),Object(a.jsx)("li",{}),Object(a.jsx)("li",{}),Object(a.jsx)("li",{})]})]})]})}})},ae=function(e){e&&e instanceof Function&&n.e(3).then(n.bind(null,289)).then((function(t){var n=t.getCLS,i=t.getFID,a=t.getFCP,c=t.getLCP,s=t.getTTFB;n(e),i(e),a(e),c(e),s(e)}))};x.b.setup({id:{minify:"production".startsWith("production")}}),Object(r.render)(Object(a.jsx)(s.a.StrictMode,{children:Object(a.jsx)(b.a,{basename:void 0,children:Object(a.jsx)(ie,{})})}),document.getElementById("root")),ae()},57:function(e,t,n){}},[[283,1,2]]]);
//# sourceMappingURL=main.2567f96b.chunk.js.map