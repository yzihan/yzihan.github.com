(this["webpackJsonpzihan-site"]=this["webpackJsonpzihan-site"]||[]).push([[0],{283:function(e,t,i){"use strict";i.r(t);var n,a=i(0),o=i(1),r=i.n(o),s=i(37),c=(i(56),i(23)),l=i(7),d=i(24),h=i(13),g=i(28),b=i(6),m=i(4),u=i.p+"static/media/placeholder.19e90386.gif",p=i.p+"static/media/loading.ccdf9915.gif",j=i(288),f=i(286),x=i(11),y=i.p+"static/media/logo.103b5fa1.svg",O=i(47),v=i(287),w=[600,800,1e3,1200,1/0],k=function(e){return w[e]},C=function(e){return e&&w[e-1]||300},S=r.a.createContext(0);function T(e){var t=e.value,i=e.args,n=e.varName,o=Object(d.a)(e,["value","args","varName"]);return Object(a.jsx)(v.a,Object(l.a)(Object(l.a)({language:"javascript",showLineNumbers:!0},o),{},{children:(n?n+" = ":"")+JSON.stringify.apply(JSON,[t].concat(Object(O.a)(i||[null,2])))}))}var N={"":function(){return Object(a.jsxs)(xe,{title:"Debug Page",children:[Object(a.jsx)(I,{}),Object(a.jsxs)("p",{children:["Width Level: ",Object(a.jsx)(S.Consumer,{children:function(e){return e.toString()}})]}),Object(a.jsx)(T,{value:Object({NODE_ENV:"production",PUBLIC_URL:"",WDS_SOCKET_HOST:void 0,WDS_SOCKET_PATH:void 0,WDS_SOCKET_PORT:void 0,FAST_REFRESH:!0}),varName:"process.env"})]})},pdf:function(){D(),Object(o.useRef)(null);var e=Object(o.useState)("/data/cv.pdf"),t=Object(c.a)(e,2),i=t[0];t[1];return Object(a.jsxs)(xe,{title:"PDF Debug",children:[Object(a.jsx)(I,{}),Object(a.jsxs)("fieldset",{children:[Object(a.jsx)("legend",{children:"PDF Data"}),void 0,Object(a.jsx)("a",{href:i,target:"_blank",rel:"noreferrer",children:"Open in new window"})]}),"Rendered:"]})},default:function(){var e=E();return Object(a.jsx)(xe,{title:"React App",className:e.outerContainer,children:Object(a.jsxs)("header",{className:e.app,children:[Object(a.jsx)("img",{src:y,className:[e.appLogo,e.appLogoSpining].join(" "),alt:"logo"}),Object(a.jsxs)("p",{children:["Edit ",Object(a.jsx)("code",{children:"src/App.tsx"})," and save to reload."]}),Object(a.jsx)(b.b,{to:"/debug/test",className:e.appLink,children:"View Test"}),Object(a.jsxs)(b.b,{to:"/debug/",className:e.appLink,children:["Back to ",Object(a.jsx)("code",{children:"/debug/"})]})]})})},test:function(){var e=E();return Object(a.jsx)(xe,{title:"Hello, React App!",className:e.outerContainer,children:Object(a.jsxs)("header",{className:e.app,children:[Object(a.jsx)("img",{src:p,className:e.appLogo,alt:"logo"}),Object(a.jsx)("p",{children:"This is test page!"}),Object(a.jsx)(b.b,{to:"/debug/default",className:e.appLink,children:"View Debug Page"}),Object(a.jsxs)(b.b,{to:"/debug/",className:e.appLink,children:["Back to ",Object(a.jsx)("code",{children:"/debug/"})]})]})})}};function A(){var e=[];for(var t in N){var i="/debug/".concat(t);e.push(Object(a.jsx)(m.b,{exact:!0,path:i,component:N[t]},i))}return e.push(Object(a.jsx)(m.b,{path:"/debug/",render:function(){return Object(a.jsx)(m.a,{to:"/debug/"})}},"wildcard")),e}function I(e){var t=[];for(var i in N){var n="/debug/".concat(i);t.push(Object(a.jsx)("li",{children:Object(a.jsx)(b.b,Object(l.a)(Object(l.a)({},e),{},{to:n,children:n}))},n))}return Object(a.jsxs)(r.a.Fragment,{children:[Object(a.jsx)("h2",{style:{marginTop:0},children:"Debug Pages"}),Object(a.jsx)(b.b,Object(l.a)(Object(l.a)({},e),{},{to:"/",children:"Back to app index"})),Object(a.jsx)("ul",{children:t})]})}var D=Object(x.a)({viewer:{maxWidth:"600px",margin:[[0,"auto"]]}});var E=Object(x.a)({"@keyframes App-logo-spin":{from:{transform:"rotate(0deg)"},to:{transform:"rotate(360deg)"}},appLogo:Object(h.a)({height:"40vmin",pointerEvents:"none"},"@media (max-width: ".concat(C(0),"px)"),{height:.4*C(0)}),appLogoSpining:{"@media (prefers-reduced-motion: no-preference)":{animation:"$App-logo-spin infinite 20s linear"}},outerContainer:{backgroundColor:"#282c34"},app:(n={minHeight:"100vh",display:"flex",flexDirection:"column",alignItems:"center",justifyContent:"center",fontSize:"calc(10px + 2vmin)"},Object(h.a)(n,"@media (max-width: ".concat(C(0),"px)"),{fontSize:10+.02*C(0)}),Object(h.a)(n,"color","white"),Object(h.a)(n,"textAlign","center"),n),appLink:{color:"#61dafb"}});Object(x.a)({outerContainer:{},page:{background:"white",color:"black",textAlign:"center",display:"flex",flexDirection:"column",justifyContent:"center",padding:[[0,30]]},infoBlock:{textAlign:"left"}});var W=i.p+"static/media/bio.61dbeff3.png",z=i.p+"static/media/material-art.670e8f9c.jpg",L=i.p+"static/media/cali1.e244eebf.jpg",P=i.p+"static/media/cali2.a5b357a8.jpg",H=i.p+"static/media/painting1.1e3a0c6c.jpg",Y=i.p+"static/media/sketch1.42ed6f25.jpg",B=i.p+"static/media/sketch2.3765bc77.jpg",F=i.p+"static/media/argaze.fb7bce3f.jpg",R=i.p+"static/media/towards.8d09635d.png",Z=i.p+"static/media/speechin.7257c7d7.jpg",M=i.p+"static/media/reducing.688cac44.png",U=i.p+"static/media/inthemaking.b7fd6a6b.png",q=i.p+"static/media/enabling.16484fd5.jpg",G=i.p+"static/media/gender.4491d26b.jpg",_=i.p+"static/media/argaze2.eecb2be9.png",J=i.p+"static/media/towards2.6021ae32.png",K=i.p+"static/media/speechin2.60718949.png",V=i.p+"static/media/reducing2.7eb70db1.png",$=i.p+"static/media/inthemaking2.a930cc79.png",X=i.p+"static/media/enabling2.c8c0a939.png",Q=i.p+"static/media/gender2.612183e8.png",ee=i.p+"static/media/bamboo.2bc0fdde.jpg",te=i.p+"static/media/tibetan.96b63814.jpg",ie=i.p+"static/media/headset.9bfeb0db.jpg",ne=i.p+"static/media/cognitive.caab71be.png",ae=i.p+"static/media/bamboo2.55ae7ba6.png",oe=[{name:"Gender Differences of Cognitive Loads in Augmented Reality-based Warehouse",shortName:"gender",previewImg:G,image:Q,writer:"Zihan Yan, Yifei Shan, Kailin Yin, Yiyang Li, Xiangdong Li",status:"Accepted",label:"Conference Paper A, IEEE VR 2021",abstract:"Augmented reality is emerging into contemporary warehouses and considerably improves human worker\u2019s cognition abilities and overall productivity. Despite proven benefits, little is known about how the male and female workers cope with cognitive loads of different warehouse tasks. Therefore, we developed the augmented reality headset to help the participants facilitate parcel sorting tasks and conducted empirical studies to investigate the gender differences of cognitive loads. The results show that the female workers had significantly lower operational efficiency, higher visual attention, and higher memory loads than the male, but they quickly gained advantages in all these aspects as fewer gender differences were identified with more difficult tasks. The female workers had higher visual attention than the male in the process of information seeking and interpretation. There were constant differences of working memory between the female and male throughout all the processes. Implications for the gender difference of cognitive loads are discussed."},{name:"SpeeChin: A Smart Necklace for Silent Speech Recognition",shortName:"speechin",previewImg:Z,image:K,writer:"Ruidong Zhang, Mingyang Chen, Benjamin Steeper, Yaxuan Li, Zihan Yan, Yizhuo Chen, Songyun Tao, Tuochao Chen, Hyunchul Lim, Cheng Zhang",status:"Submitted",label:"MobiSys 2021",abstract:"This paper presents SpeeChin, a smart necklace that can recognize 54 English and 44 Chinese silent speech commands. A customized infra-red (IR) imaging system is mounted on a necklace to capture images of the neck and face from under the chin. These images are first pre-processed and then deep learned by an end-to-end deep convolutional-recurrent-neural-network (CRNN) model to infer different silent speech commands. A user study with 20 participants (10 participants for each language) showed that SpeeChin can recognize 54 English and 44 Chinese silent speech commands with average cross-session accuracies of 90.6% and 92.1%respectively. To further investigate the potential of SpeeChinin recognizing other silent speech commands, we conducted another study with 10 participants distinguishing between 72one-syllable nonwords. Based on the results from the user studies, we further discuss the challenges and opportunities of deploying SpeeChin in real-world applications."},{name:"ARGaze: A Dataset of Eye Gaze Images for Calibration-Free Eye Tracking with Augmented Reality Headset",shortName:"argaze",previewImg:F,image:_,writer:"Zihan Yan, Yue Wu, Yifei Shan, Wenqian Chen, Xiangdong Li",status:"Submitted",label:"Journal of Nature-Scientific Data",abstract:"Eye-tracking is a widespread method in human-computer interaction. However, it is often criticised for the troublesome calibration with new users and scenes. Despite progress in machine learning-based eye tracking, preparing a qualified dataset remains challenging. We present ARGaze, a dataset of eye gaze images, for calibration-free eye tracking with AR headset. The dataset was derived from 25 participants who conducted eye gaze tasks in augmented reality and real-world scenes for approximately 30min. It comprises 1,321,968 pairs of eye images and corresponding world view in 50 videos. To validate the dataset, we implemented the SIFTNet- and ALSTM-FCN-hybrid model and compared the results with that of the state-of-the-art research. The results show that the dataset is of high compatibility with different machine learning models and it contains sufficient eye gaze-related features that enable the record low eye gaze estimation error by 3.70degree on average and 1.56degree on specific participant, without involving any pre-study calibrations across the participants. Guidance for dataset reuse and related implications for eye tracking design and evaluation are described."},{name:"Reducing Cognitive Loads in Parcel Scanning with Eye Tracking-based Augmented Reality Headset",shortName:"reducing",previewImg:M,image:V,writer:"Zihan Yan, Yufei Wu, Yiyang Li, Yifei Shan, Preben Hansen, Xiangdong Li",status:"Submitted",label:"Interacting with Computers",abstract:"Parcel scanning in warehouse is a highly frequent task that consists of multiple processes e.g. barcode seeking and scanning, scan result confirming, and parcel relocating. It is cognitively demanding as each process involves a respective level of cognitive loads and fluctuation of cognitive loads would quickly deteriorate human workers\u2019 productivity. Despite many wearable devices were devised for efficiency of parcel scanning, few are aimed at eliminating cognitive loads. We developed the eye tracking-based augmented reality headset with foveated vision detection and smooth pursuit to leverage parallel parcel barcode seeking-scanning and spontaneous scan result confirming, respectively. We recruited 33 participants to investigate how the headset influenced cognitive loads. The results show that the headset maintained high scanning efficiency and lower cognitive loads across the tasks with varying difficulties and it significantly reduced the participants\u2019 cognitive loads during the processes of barcode seeking and scanning and result confirmation. The headset also demonstrated good usability and ease of use."},{name:"Towards moment-to-moment attention-aware interfaces by detecting subsecond-scale attention fluctuations through EEG",shortName:"towards",previewImg:R,image:J,writer:"Shan Zhang*, Zihan Yan*, Shardul Sapkota, Shengdong Zhao, Wei-Tsang Ooi, Ye Qiyuan",status:"In Preparation",label:"Sensors MDPI",abstract:"Moment-to-moment attention fluctuation can bring unwanted and even life-threatening outcomes. While there are previous studies pertaining to attention detection, none have studied sub-second-scale moment-to-moment attention fluctuations. To enable automatic detection of this occurrence, we utilized a novel paradigm in psychology\u2014 the gradCPT, which measures attention through response time variability measurements and classifies sub-second-scale moment-to-moment attention fluctuations as \u201cin-the-zone\u201d and \u201cout-of-the-zone\u201d. We developed an EEG-based classifier to classify both states with an accuracy of 73.4%. We then evaluated our classifier through a thought probe study in the video learning scenario. The probe study showed that our EEG-based classifier can classify on-task and mind-wandering during video learning and improve the state-of-the-art by 9%. The results demonstrated the effectiveness of EEG-based detection of moment-to-moment attention fluctuation, which can be used to develop more attention aware systems in other real-world scenarios."},{name:"In the Making of Eye Tracking-enabled Augmented Reality Headset",shortName:"inthemaking",previewImg:U,image:$,writer:"Xiangdong Li, Yue Wu, Yifei Shan, Zihan Yan, Wenqian Chen, Qiuyi Yang",status:"Submitted",label:"Multimedia Tools and Applications",abstract:"The changes of mixed reality technology are happening in the industry at a record pace and eye tracking has become a new frontier in understanding user\u2019s attentional behaviours in mixed reality context. Despite several mixed reality systems integrated eye tracking add-on, few of these studies systematically tackled ease of assembly of the devices. Therefore, we developed the ETGaze, an ease of assembly eye tracking device for mixed reality study and evaluated its ease of assembly with respect to perceived usability, reliability, and generalisability. The results show that users took 22.87sec approximately for full assembly of the device, which is 2.9 times faster than the time of assembling the Holokit. The participants reported 8.27 out of 10 overall satisfaction and over 80% reported the ETGaze assembly was easier than Holokit. The device reached 1.02degree and 1.30degree estimation error in augmented reality and real-world scenes, respectively. Implications for how generalising the device could be used in other mixed reality contexts are discussed."},{name:"Enabling Asynchronous Collaboration of Exhibit Browsing in Augmented Reality Museum.",shortName:"enabling",previewImg:q,image:X,writer:"Xiangdong Li, Wenqian Chen, Yifei Shan, Yue Wu, Zihan Yan",status:"Submitted",label:"IEEE Access",abstract:"The augmented reality museums allow multiple visitors to jointly view and interact with exhibit by sharing a digital instance of the exhibit. However, real-time collaboration does not accommodate latecomers to museum and in general it offers limited support for temporally separated visitors. To facilitate all visitors participating at different times, we divide the exhibit-egocentric space into four distance ranges, and each is assigned with a set of social networking features that are associated with different privileges of exhibit information display and interaction. We conduct empirical studies on how the asynchronous collaboration tool affects visitors\u2019 exhibit browsing with respect to perceived usability and learning gains. The results show that the tool has enabled consistent perceived usability to all the participants standing at different distances through discriminative information display. Furthermore, the model has significantly enhanced the participants\u2019 learning efficiency and learning attention durations by stimulating the participants\u2019 awareness of multiuser communication. Implications for how the asynchronous collaboration tool may be generalisingly used in other augmented reality applications are drawn."}];function re(){var e=se();return Object(a.jsxs)(a.Fragment,{children:[Object(a.jsx)("h3",{children:"International Competition"}),Object(a.jsxs)(pe,{src:ee,alt:"bamboo",children:[Object(a.jsx)(b.b,{to:"/design/bamboo.html","data-stay-black":!0,children:Object(a.jsx)("h3",{children:"Bamboo Shoot (A Soil Remediation Product Used Industrial Wastes)"})}),Object(a.jsx)("p",{children:Object(a.jsx)("span",{className:[e.labels,e.cyan].join(" "),children:"iF Design Talent Award 2020"})})]}),Object(a.jsx)("h3",{children:"Patents for Invention"}),Object(a.jsxs)(pe,{src:te,alt:"tibetan",children:[Object(a.jsx)("h3",{children:"A Tibetan Dance Shoe with Pattern Projection Function"}),Object(a.jsx)("p",{children:Object(a.jsx)("span",{className:[e.labels,e.red].join(" "),children:"CN 110710755 A"})}),Object(a.jsx)("p",{})]}),Object(a.jsxs)(pe,{src:ie,alt:"headset",children:[Object(a.jsx)("h3",{children:"An Eye-tracking based calibration-free AR headset design for picking task in warehouse"}),Object(a.jsx)("p",{children:Object(a.jsx)("span",{className:[e.labels,e.orange].join(" "),children:"Pending"})})]}),Object(a.jsxs)(pe,{src:ne,alt:"cognitive",children:[Object(a.jsx)("h3",{children:"Cognitive Load and Fatigue Detection Method and Device in Order Picking Tasks."}),Object(a.jsx)("p",{children:Object(a.jsx)("span",{className:[e.labels,e.orange].join(" "),children:"Pending"})})]})]})}var se=Object(x.a)({labels:{display:"inline-block",borderRadius:3,border:"1px solid #333",padding:"3px 8px",fontSize:"small"},green:{color:"#52c41a",background:"#f6ffed",borderColor:"#b7eb8f"},orange:{color:"#fa8c16",background:"#fff7e6",borderColor:"#ffd591"},gold:{color:"#faad14",background:"#fffbe6",borderColor:"#ffe58f"},cyan:{color:"#13c2c2",background:"#e6fffb",borderColor:"#87e8de"},red:{color:"#f5222d",background:"#fff1f0",borderColor:"#ffa39e"}});function ce(){var e=se();return Object(a.jsx)(a.Fragment,{children:oe.map((function(t){return Object(a.jsxs)(pe,{src:t.previewImg,alt:t.shortName,children:[Object(a.jsx)(b.b,{to:"/publication/".concat(t.shortName,".html"),"data-stay-black":!0,children:Object(a.jsx)("h3",{children:t.name})}),Object(a.jsx)("p",{children:t.writer.split("Zihan Yan",2).reduce((function(e,t){return"string"===typeof e?[e,Object(a.jsx)("span",{style:{fontWeight:"bold"},children:"Zihan Yan"},"name"),t]:e.concat([Object(a.jsx)("span",{style:{fontWeight:"bold"},children:"Zihan Yan"},"name"),t])}))}),Object(a.jsxs)("p",{children:[Object(a.jsx)("span",{className:[e.labels,e.green].join(" "),children:t.label}),Object(a.jsx)("span",{style:{float:"right"},className:[e.labels,"Accepted"===t.status?e.red:e.gold].join(" "),children:t.status})]})]},t.shortName)}))})}var le,de=Object(l.a)(Object(l.a)({"/":{title:"Home",shortTitle:"Home",component:function(){return Object(a.jsxs)(xe,{title:"Home",children:[Object(a.jsx)("h2",{children:"About me"}),Object(a.jsxs)(pe,{src:W,alt:"bio",allowLargeImage:!0,children:[Object(a.jsx)("h3",{children:"Hi! You can call me Zihan"}),Object(a.jsxs)("p",{children:["I am an undergraduate from Zhejiang University, major in industrial design of Computer Science College and minor in the advanced education engineering class of CHU KOCHEN Honors college. My research interests lie in ",Object(a.jsx)("b",{children:"human-computer interaction"})," (augmented reality, cross-object/screen interaction), ",Object(a.jsx)("b",{children:"ubiquitous computing"})," (deep learning, sensing wearable devices), and ",Object(a.jsx)("b",{children:"cognitive psychology"})," (cognitive load, mind wandering). Coming from multi-disciplinary backgrounds, I pursue to further facilitate human-to-human communication by combining machine intelligence and artificial intelligence."]}),Object(a.jsxs)("p",{children:["Presently, I am a research intern at ",Object(a.jsx)("a",{href:"https://www.scifilab.org/",children:"SciFi Lab"})," of ",Object(a.jsx)("b",{children:"Cornell University"})," advised by ",Object(a.jsx)("a",{href:"http://www.czhang.org/",children:"Dr. Cheng Zhang."})," During the summer vacation of my sophomore year, I was previously a student intern at ",Object(a.jsx)("a",{href:"http://www.nus-hci.org/",children:"NUS-HCI Lab"})," of ",Object(a.jsx)("b",{children:"National University of Singapore"})," under the guidance of ",Object(a.jsx)("a",{href:"http://www.shengdongzhao.com",children:"Dr. Shengdong Zhao"}),". In addition, I was a research assistant in CDC-Lab of Zhejiang University and completed some researches with the supervisor ",Object(a.jsx)("a",{href:"https://person.zju.edu.cn/en/andolxli",children:"Dr. Xiangdong Li"})," and co-advisor ",Object(a.jsx)("a",{href:"https://hansen.blogs.dsv.su.se/",children:"Dr. Preben Hansen"})," from Stockholm University."]})]}),Object(a.jsx)("h2",{children:"Publication"}),Object(a.jsx)(ce,{}),Object(a.jsx)("h2",{children:"Design"}),Object(a.jsx)(re,{})]})}},"/cv.html":{title:"CV",shortTitle:"CV",component:function(){return Object(a.jsx)(xe,{title:"CV",children:Object(a.jsx)("iframe",{src:"/data/Zihan_CV.pdf",style:{width:"100%",height:"90vh"}})})}},"/publication/":{title:"Publication",shortTitle:"Publication",component:function(){return Object(a.jsx)(xe,{title:"Publications",children:Object(a.jsx)(ce,{})})}}},function(e){for(var t={},i=function(){var i=o[n];t["".concat(e).concat(i.shortName,".html")]={component:function(){return Object(a.jsxs)(xe,{title:i.name,children:[Object(a.jsx)(b.b,{to:e,children:"Show All Publications"}),Object(a.jsx)("h2",{children:i.name}),Object(a.jsx)("p",{children:i.writer}),Object(a.jsx)("img",{style:{maxWidth:"100%"},src:i.image,alt:i.shortName}),Object(a.jsx)("h3",{children:"Abstract"}),Object(a.jsx)("p",{style:{textAlign:"justify"},children:i.abstract}),Object(a.jsx)(b.b,{to:e,children:"Show All Publications"})]})}}},n=0,o=oe;n<o.length;n++)i();return t}("/publication/")),{},{"/design/":{title:"Design",shortTitle:"Design",component:function(){return Object(a.jsxs)(xe,{title:"Design",children:[Object(a.jsx)("h2",{children:"Design"}),Object(a.jsx)(re,{})]})}},"/design/bamboo.html":{component:function(){return Object(a.jsxs)(xe,{title:"Design",children:[Object(a.jsx)(b.b,{to:"/design/",children:"Show All Designs"}),Object(a.jsx)("h3",{children:"Bamboo Shoot (A Soil Remediation Product Used Industrial Wastes)"}),Object(a.jsx)("p",{children:"Qianya Lou, Jingchen An, Kaiqi Jiang, Zihan Yan"}),Object(a.jsx)("img",{style:{maxWidth:"100%"},src:ae,alt:"bamboo2"}),Object(a.jsx)("h4",{children:"Introduction"}),Object(a.jsx)("p",{children:"Bamboo shoots, which give people the hope of green life by the upward growth posture, is a design to deal with heavy metals and organic soil pollution. Nowadays, the standard rate of the national total soil point exceeding is 16.1%, so the soil recovery needs to be highlighted."}),Object(a.jsx)("h4",{children:"Jury Statement"}),Object(a.jsx)("p",{children:"Bamboo Shoot is an interesting proposal with great application potential. The proposal solves several problems related to soil restoration."}),Object(a.jsx)(b.b,{to:"/design/",children:"Show All Designs"})]})}},"/hobby.html":{title:"Hobby",shortTitle:"Hobby",component:function(){return Object(a.jsxs)(xe,{title:"Hobby",children:[Object(a.jsx)("h2",{children:"Martial Art"}),Object(a.jsx)("p",{children:"Member of Chinese Martial Arts Association"}),Object(a.jsx)("img",{style:{maxWidth:"100%"},src:z,alt:"my material art skill"}),Object(a.jsxs)("ul",{children:[Object(a.jsx)("li",{children:"The 14th Hong Kong International Martial Arts Competition, First prize of Traditional Bajiquan (2019)"}),Object(a.jsx)("li",{children:"The 14th Hong Kong International Martial Arts Competition, First prize of Pictographic boxing (2019)"}),Object(a.jsx)("li",{children:"Chinese College Students Martial Arts Championship, Fourth prize of Soft Instruments (2019)"}),Object(a.jsx)("li",{children:"Chinese College Students Martial Arts Championship, Fourth prize of Pictographic boxing (2019)"})]}),Object(a.jsx)("h2",{children:"Calligraphy"}),Object(a.jsx)("img",{style:{maxWidth:"39.29%"},src:L,alt:"my calligraphy #1"}),Object(a.jsx)("img",{style:{maxWidth:"60.71%"},src:P,alt:"my calligraphy #2"}),Object(a.jsx)("h2",{children:"Painting"}),Object(a.jsx)("h3",{children:"Watercolour"}),Object(a.jsx)("img",{style:{maxWidth:"100%"},src:H,alt:"my painting #1"}),Object(a.jsx)("h3",{children:"Pancel Sketch"}),Object(a.jsx)("img",{style:{maxWidth:"50.76%"},src:Y,alt:"my sketch #1"}),Object(a.jsx)("img",{style:{maxWidth:"49.24%"},src:B,alt:"my sketch #2"})]})}}}),he=i(45),ge=i(32),be=i(46);he.a.add(ge.b,ge.a,be.a);var me=[p,u],ue=Object(x.a)({imgContainer:(le={width:"40%",float:"left",overflow:"hidden",marginRight:20},Object(h.a)(le,"@media (max-width: ".concat(k(0),"px)"),{display:"block",float:"none",width:"100%",maxWidth:1*C(0),margin:[[0,"auto"]]}),Object(h.a)(le,"&:not($largeImage)",{maxWidth:.66666*C(0),outline:"1px solid #333",outlineOffset:0}),le),imgSubContainer:{width:"100%",position:"relative",height:0,padding:0,paddingBottom:"65%","$largeImage &":{paddingBottom:"100%"}},img:{objectFit:"cover",position:"absolute",height:"100%",minHeight:"100%",minWidth:"100%"},largeImage:{},container:{marginTop:10,marginBottom:10,clear:"both","&>p":{textAlign:"justify",textAlignLast:"auto"},"&::after":{content:'""',display:"block",clear:"both"}}});function pe(e){var t=ue(),i=e.className,n=e.children,o=e.alt,r=e.allowLargeImage,s=e.containerStyle,c=Object(d.a)(e,["className","children","alt","allowLargeImage","containerStyle"]);return Object(a.jsxs)("div",{className:t.container,style:s,children:[Object(a.jsx)("div",{className:[t.imgContainer,r?t.largeImage:void 0].join(" "),children:Object(a.jsx)("div",{className:t.imgSubContainer,children:Object(a.jsx)("img",Object(l.a)({className:[i,t.img,r?t.largeImage:void 0].join(" "),alt:o},c))})}),n]})}var je=Object(x.a)({"@global":{body:{minWidth:C(0)},b:{fontWeight:700},a:{color:"inherit","main &":{color:"#3c88d4",textDecoration:"none",transition:"color 0.5s linear","&[data-stay-black]":{color:"#000000"},"&[data-stay-black]:hover":{color:"#e53935"},"&:hover":{color:"#e53935"}}}},originalName:{fontSize:"0.6em",fontWeight:"normal",display:"inline-block",margin:10,fontStyle:"normal","&:before":{content:'"("',display:"inline"},"&:after":{content:'")"',display:"inline"}},"@keyframes gradient":{from:{backgroundPosition:[["0%","50%"]]},"50%":{backgroundPosition:[["100%","50%"]]},to:{backgroundPosition:[["0%","50%"]]}},header:{background:"linear-gradient(-45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab)",color:"white",backgroundSize:[["400%","400%"]],animation:"$gradient 15s ease infinite",padding:40,"& h1, & p":{margin:0,padding:0}},stuck:{},navBar:{transition:"0.2s","& ul":{display:"flex",flexFlow:[["row","wrap"]],listStyleType:"none",padding:0,margin:0},"& li":{display:"inline-block"},"&:not($navBarFloating)":{background:"transparent",margin:[[20,0,-40,-20]],opacity:1,"$stuck&":{opacity:0}}},navBarFloating:{position:"fixed",left:0,top:0,right:0,zIndex:99999,background:"#000000",opacity:0,"$stuck&":{opacity:1}},navButton:{cursor:"pointer",transition:"0.1s",background:"transparent",display:"inline-block",padding:[[15,20]],"&:hover":{background:"rgba(204,204,204,0.5)"}},framework:{},footer:{background:"linear-gradient(to bottom right, #50a3a2 0%, #53e3a6 100%)",fallback:{background:"#50a3a2"},color:"white",position:"relative",padding:40,display:"flex",justifyContent:"space-between","&>div":{display:"block"},"& h3, & p":{marginTop:0}},footerBg:{position:"absolute",top:0,left:0,width:"100%",height:"100%",zIndex:1,pointerEvents:"none",margin:0,padding:0,overflow:"hidden","& li":{position:"absolute",listStyle:"none",display:"block",width:40,height:40,backgroundColor:"rgba(255,255,255,0.15)",bottom:-160,animation:"$square 25s infinite",transitionTimingFunction:"linear","&:nth-child(1)":{left:"10%"},"&:nth-child(2)":{left:"20%",width:80,height:80,animationDelay:"2s",animationDuration:"17s"},"&:nth-child(3)":{left:"25%",animationDelay:"4s"},"&:nth-child(4)":{left:"40%",width:60,height:60,animationDuration:"22s",backgroundColor:"rgba(255,255,255,0.25)"},"&:nth-child(5)":{left:"70%"},"&:nth-child(6)":{left:"80%",width:120,height:120,animationDelay:"3s",backgroundColor:"rgba(255,255,255,0.20)"},"&:nth-child(7)":{left:"32%",width:160,height:160,animationDelay:"7s"},"&:nth-child(8)":{left:"55%",width:20,height:20,animationDelay:"15s",animationDuration:"40s"},"&:nth-child(9)":{left:"25%",width:10,height:10,animationDelay:"2s",animationDuration:"40s",backgroundColor:"rgba(255,255,255,0.30)"},"&:nth-child(10)":{left:"90%",width:160,height:160,animationDelay:"11s"}}},"@keyframes square":{from:{transform:"translateY(0)"},to:{transform:"translateY(-700px) rotate(600deg)"}},pageContainer:{position:"relative",transition:"0.3s"},pageEnter:{position:"absolute",left:0,right:0,top:0,opacity:1},pageExit:{position:"absolute",left:0,right:0,top:0,opacity:0}}),fe=Object(x.a)({outerContainer:{transition:"0.3s",minHeight:"30vh",fallback:{minHeight:"300px"},padding:20},pageContentWrapper:{maxWidth:k(2),margin:[[0,"auto"]]}});function xe(e){var t=fe(),i=e.title,n=e.className,o=e.children,r=Object(d.a)(e,["title","className","children"]);return Object(a.jsx)("div",Object(l.a)(Object(l.a)({className:[t.outerContainer,n].join(" ")},r),{},{children:Object(a.jsxs)("div",{className:t.pageContentWrapper,children:[Object(a.jsx)(g.a,{title:i}),o]})}))}var ye=function(){var e=je(),t=Object(o.useState)(document.body.clientWidth),i=Object(c.a)(t,2),n=i[0],r=i[1],s=Object(o.useRef)(null),l=Object(o.useState)(!1),d=Object(c.a)(l,2),h=d[0],u=d[1];Object(o.useEffect)((function(){var e=function(e){r(document.body.clientWidth)};return window.addEventListener("resize",e),function(){return window.removeEventListener("resize",e)}})),Object(o.useEffect)((function(){var e=function(){s.current&&u(s.current.getBoundingClientRect().top<=1)};e();var t=function(e){var t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:100,i=null;return function(){for(var n=arguments.length,a=new Array(n),o=0;o<n;o++)a[o]=arguments[o];i||(i=setTimeout((function(){e.apply(void 0,a),i=null}),t))}}(e,50);return window.addEventListener("resize",t),window.addEventListener("scroll",t),function(){window.removeEventListener("resize",t),window.removeEventListener("scroll",t)}}));var p,x=(p=n,w.findIndex((function(e){return p<=e}))),y=["Collage of Computer Science","Collage of Computer Science | Zhejiang University","Collage of Computer Science and Technology | Zhejiang University","Collage of Computer Science and Technology | Zhejiang University","Collage of Computer Science and Technology | Zhejiang University"],O=x<1?"shortTitle":"title",v=Object(a.jsx)("ul",{children:Object.keys(de).filter((function(e){return!!de[e].title})).map((function(t){return Object(a.jsx)("li",{children:Object(a.jsx)(b.b,{to:t,className:e.navButton,children:de[t][O]})},t)}))});return Object(a.jsx)(m.b,{render:function(t){var i=t.location;return Object(a.jsxs)(S.Provider,{value:x,children:[Object(a.jsxs)("div",{className:e.framework,children:[Object(a.jsx)(g.a,{children:me.map((function(e){return Object(a.jsx)("link",{rel:"prefetch",href:e},e)}))}),Object(a.jsxs)("header",{className:e.header,children:[Object(a.jsx)("h1",{children:"Zihan Yan "}),Object(a.jsx)("p",{children:y[x]}),Object(a.jsx)("nav",{ref:s,className:[e.navBar,h?e.stuck:void 0].join(" "),children:v}),Object(a.jsx)("nav",{className:[e.navBar,e.navBarFloating,h?e.stuck:void 0].join(" "),children:v})]}),Object(a.jsx)(j.a,{component:"main",className:e.pageContainer,children:Object(a.jsx)(f.a,{classNames:{enter:e.pageEnter,exit:e.pageExit},timeout:1,children:Object(a.jsxs)(m.d,{location:i,children:[A(),Object.keys(de).map((function(e){return Object(a.jsx)(m.b,{exact:!0,path:e,component:de[e].component},e)})),Object(a.jsx)(m.b,{render:function(){return Object(a.jsx)(m.a,{to:"/"})}})]},i.pathname)},i.pathname)})]}),Object(a.jsxs)("footer",{className:e.footer,children:[Object(a.jsxs)("div",{children:[Object(a.jsx)("h3",{children:"Contact Information"}),Object(a.jsx)("p",{children:Object(a.jsx)("a",{href:"mailto:zihanyan@zju.edu.cn",children:"zihanyan@zju.edu.cn"})})]}),Object(a.jsx)("div",{style:{display:void 0},children:Object(a.jsx)("p",{children:"\xa9 2020 Zihan"})}),Object(a.jsxs)("ul",{className:e.footerBg,style:{clear:"both"},children:[Object(a.jsx)("li",{}),Object(a.jsx)("li",{}),Object(a.jsx)("li",{}),Object(a.jsx)("li",{}),Object(a.jsx)("li",{}),Object(a.jsx)("li",{}),Object(a.jsx)("li",{}),Object(a.jsx)("li",{}),Object(a.jsx)("li",{}),Object(a.jsx)("li",{})]})]})]})}})},Oe=function(e){e&&e instanceof Function&&i.e(3).then(i.bind(null,289)).then((function(t){var i=t.getCLS,n=t.getFID,a=t.getFCP,o=t.getLCP,r=t.getTTFB;i(e),n(e),a(e),o(e),r(e)}))};x.b.setup({id:{minify:"production".startsWith("production")}}),Object(s.render)(Object(a.jsx)(r.a.StrictMode,{children:Object(a.jsx)(b.a,{basename:void 0,children:Object(a.jsx)(ye,{})})}),document.getElementById("root")),Oe()},56:function(e,t,i){}},[[283,1,2]]]);
//# sourceMappingURL=main.ec3b46b6.chunk.js.map